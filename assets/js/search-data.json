{
  
    
        "post0": {
            "title": "Rust Async",
            "content": ". Some definitions . . green threads: threads scheduled by vm or runtime | native threads are scheduled by os | runtime: the env where your code runs and the libraries it has access to (e.g. jvm, stdlib, malloc). | . . Why use async over OS-provided threads . . native threads are expensive | the async runtime creates its own green threads from the os/kernel and schedules access to them | it handles keeping track of the state of async functions | . “It is registering incoming Future requests and saves a pointer to the async function handler. It then triggers an event in the kernel. Once the I/O operation is done, we call the pointer and execute the async method with the results from the I/O (kernel). For this, we need a Reactor, which notifies if data is coming over the network or a file writing operation is in progress, and an executor which takes this data and executes the async function (Future) with it.” . . https://manishearth.github.io/blog/2018/01/10/whats-tokio-and-async-io-all-about/ (outdated?) | . “You can wait() on a Future, which will block until you have a result, and you can also poll() it, asking it if it’s done yet (it will give you the result if it is).” . “You have to manually set up the Tokio event loop (the “scheduler”), but once you do you can feed it tasks which intermittently do I/O, and the event loop takes care of swapping over to a new task when one is blocked on I/O” . Resouces . https://manishearth.github.io/blog/2018/01/10/whats-tokio-and-async-io-all-about/ | https://rust-lang.github.io/async-book/01_getting_started/01_chapter.html | https://levelup.gitconnected.com/explained-how-does-async-work-in-rust-c406f411b2e2 | https://softwareengineering.stackexchange.com/questions/304427/what-really-is-the-runtime-environment | https://areweasyncyet.rs/ | http://www.arewewebyet.org/ | https://tokio.rs/ | .",
            "url": "https://bkkaggle.github.io/blog/rust/2020/08/14/rust-async.html",
            "relUrl": "/rust/2020/08/14/rust-async.html",
            "date": " • Aug 14, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Algpt2 Part 2",
            "content": ". Part 1: Best Practices for Finetuning Large Transformer Language models . Part 2: How I (almost) replicated OpenAI’s GPT-2 (124M version) . . TL;DR . . A few months ago I started working on a research project trying to pretrain my own, more efficient language model from scratch. I got access to a 128-core TPUv3 pod from the Tensorflow Reseach Cloud and used it to pretrain a $124$M parameter GPT-2 model to a perplexity pretty close to OpenAI’s results (my pretrained model was trained for about $1/8$th of the number of iterations that OpenAI trained their model for and got $21$ ppl on OpenWebText compared to $17$ ppl for OpenAI’s model), and then pretrained an ALBERT-style GPT-2 (that I’m calling ALGPT2) language model with a factorized input embedding and layer-wise parameter sharing that would reduce the number of paramters in the model from $124$M to around $12$M. . Unfortunately, ALGPT-2 doesn’t perform as well as GPT-2 (ALGPT-2 gets $31$ ppl on OpenWebText compared to $21$ ppl for my pretrained GPT-2 model), but I’m writing this series of blog posts to go through everything I’ve learned over the last few months. . . The Idea . . The main thing that I wanted to do from this sort-of “research project” that I was working on by myself this spring was to develop and train a more efficient version of the $124$M parameter version of GPT-2. I wanted to pretrain the $1.5$B parameter version of GPT-2 but since I only got access to the TPU pod for a week, I had to choose a model that would train in time. A $100$k iteration training run takes about $20$ hours to run which gave me plenty of time to run multiple experiments. In contrast, following OpenAI’s training procedure exactly and training for the full $800$k iterations would take up almost an entire week and use up most of my quota. . I was able to almost replicate the $124$M parameter version of GPT-2 by pretraining it to a perplexity pretty close to OpenAI’s results (my pretrained model used was trained for about $1/8$th of the number of iterations that OpenAI trained their model for and got $21$ perplexity (ppl) on the standard OpenWebText dataset compared to $17$ ppl for OpenAI’s model), . My idea of making a more efficient transformer didn’t really work out since my pretrained transformer ended up being about $20$ppl worse than an equivalent GPT-2 model, but I wanted to writeup what I learned over the two or three months that I was working on this anyway. . . Background . . A little bit about myself: I’m an incoming software engineering student at the University of Waterloo and this post is supposed to be a writeup of a NLP research project that I was working on from around March to May of 2020 (right in the middle of the first Covid-19 lockdown of 2020, I’m currently writing this on July 15, 2020 while waiting for my Pix2PixHD model to train for a few hundred epochs on colab for a new project that I’m working on). . Over the last three or four years I’ve done a lot of machine learning related stuff. I started out back in early 2017 by going through the Introduction to Machine Learning with Python and Hands-On Machine Learning with Scikit-Learn and TensorFlow books. At the time, I didn’t really understand all the math behind neural networks, but it got me hooked on ML and then I took the deeplearning.ai courses on Coursera and the original fast.ai course (back in late 2017 when they hadn’t switched over to Pytorch and still used Tensorflow and Keras). . I started competing on Kaggle in early 2018 and kept on competing in competitions non-stop for about a year and a half, winning a few medals and becoming a competitions expert (At one point I was ranked in the top $100$ Kagglers on the competitions leaderboard). Kaggle was a really nice way to get a lot of experience using neural networks because of the wide range of competitions and datasets that I had access to. I started out by doing a few semantic segmentation competitions then moved onto competing in NLP competitions. Since around mid 2019, I’ve been working on a bunch of different projects in ML and lower-level CS stuff. I worked on making a PyTorch-style machine learning library in C++ and more recently in Rust, and for the last few months I’ve also been trying to keep up with all the new machine learning (esp. NLP) papers on arXiv. . I was pretty lucky that I started learning NLP right before transformers exploded in popularity, I remember when word2vec and LSTMs were still SOTA on a lot of NLP tasks, and it has been really interesting to see how much the field of NLP has changed in just a few years, going from when LSTMs with only a a handful of layers and somewhere on the order of $512$ units were considered to be large networks and computationally expensive to train, to training LSTMs with attention layers on top, to the original transformer encoder/decoder networks, to ULMFIT and ELMO, then BERT, RoBERTa, GPT-2, and T5, to just a few months ago with the explosion of new, more efficient replacements for self-attention like the Sparse Transformer, the Reformer, and Synthesizers, and now GPT-3, which IMO has the potential to really change the whole field of NLP. . Just a few years ago we trained shallow recurrent networks on datasets, then pretrained large transformer language models on large datasets and finetuned on task-specific datasets. Now the whole idea of just training a gigantic language model on a huge dataset, then conditioning the model in a form of few-shot learning by prepending a few examples of a certain task to an input feels like it can really make NLP models a lot more accessible and easier to productionize as well as making human-chatbot interactions a lot more realistic than they are today. . I’ve rambled on for long enough, lets get to the main topic of this post. . . GPT-2 and ALBERT . . GPT-2 is a transformer decoder. . The embedding layer at the root of the model maps a one-hot vector of a given token’s index (all the GPT-2 models use a vocabulary size of $50257$) to a $768$ dimensional vector (all GPT-2 numbers in this blog post will be for the $124$m parameter version of GPT-2). . The embedding matrix is followed by a stack of self-attention and feed-forward layers that each output a $768$ dimensional vector (keeping the number of outputs for each layer constant), which makes up the main part of the transformer. . The stack of self-attention layers is then followed by an output embedding (the weights of the input and output embeddings are tied to make training easier) that maps the $768$ dimensional vector that is the output of the last layer of the transformer to the same $50257$ dimensional vector that represents the probability of each token in the vocabulary being the next token in the sequence. . Take a look at The Illustrated GPT-2 for a more in-depth look into GPT-2. . ALBERT (A Lite BERT) is a paper that takes a look at BERT and identifies some ways in which to make it more efficient and reduce the number of parameters in the model in four major ways: a factorized embedding, layer-wise parameter sharing, a sentence-order-prediction auxillary loss, and removing dropout. . . Factorized embedding . . GPT-2’s embedding has a lot of parameters. It’s really just a matrix of dimensions $50257 times 768$. That means that the input embedding alone uses up almost $50257 times 768 = space sim 38,000,000$ parameters which is a pretty big chunk of the $128$M total parameters in the model. . The ALBERT authors propose a factorized embedding with an intermediate embedding size of $128$: one embedding of size $50257 times 128$ and another embedding of size $128 times 768$. By breaking up the large embedding matrix into two smaller matrices, the total number of parameters used in the embedding goes from about $38$M to about $6$M. . $50257 times 128 = sim 6,000,000$ . $128 times 768 = sim 100,000$ . The authors try different intermediate embedding sizes and settle on $128$ as a good tradeoff betweeen parameters and performance. . . Layer-wise parameter sharing . . In a normal transformer model, the transformer layers are created something like this: . class BERT(nn.Module): def __init__(self, n_layers): super().__init__() // ... self.blocks = nn.ModuleList([Block() for _ in range(n_layers)]) // ... def forward(self, x): // ... for block in self.blocks: x = block(x) // ... . ALBERT shares all parameters across the transformer layers something like this: . class ALBERT(nn.Module): def __init__(self, n_layers): super().__init__() // ... self.n_layers = n_layers self.block = Block() // ... def forward(self, x): // ... for _ in self.n_layers: x = block(x) // ... . By only defining one transformer block and looping around it n_layers times, ALBERT saves the GPU memory that would be used to store the parameters for all the layers. . Since we normally use $32$ bit floats to store parameters on the GPU, storing the $1.5$B parameter GPT-2 on the GPU will use up about $6$GB of the GPU’s memory — that’s a pretty big chunk of the $16$GB of memory that’s on a normal V100 GPU already being used up before taking into account the memory needed to store the model’s activations as well as any momentum parameters needed by the optimizer. In contrast, if you share parameters across all transformer layers in the $1.5$B parameter GPT-2, the resulting model will only have about $37$M parameters, the parameter-sharing version would only use up around $148$MB of GPU memory. . The authors try applying parameter sharing to BERT and see that it reduces performance but makes it easier to train larger and larger models. . In a machine learning framework like JAX, which by default unrolls and inlines loops when it’s compiling your code with XLA, the size of the unrolled and inlined loop would make the computation graph really large and take a long time to compile. This is why you’re recommended to use somehting like lax.scan() in these situations. . . Sentence-order-prediction auxillary loss . The ALBERT authors add an auxillary loss to help training. Since language modelling is usually done autoregressively, I didn’t use this for my custom model. . . Removing dropout . The ALBERT authors remove all dropout from BERT and see that it significantly improves performance. . . That’s pretty much what my idea was: Take GPT-2, add a factorized embedding, share parameters across all transformer layers, remove dropout (I actually missed the part about ALBERT removing dropout until I was pretty far into my work, but I did run one or two runs without dropout to see how that works), and pretrain on a large dataset for a few hundred thousand iterations. . There’s no way that I could pretrain something like GPT-2 by myself, so I applied to the Tensorflow Research Cloud (TFRC). . The TFRC puts an emphasis on wanting to help researchers from non-traditional backgrounds which makes it an amazing resource for anyone who isn’t a “traditional” machine learning researcher. They were willing to give me, a 17 year old with no formal education or credentials (not even a high school diploma :/), access to an extremely powerful cluster of TPUs at no cost. Being able to be a part of this program was really helpful to me, especially since I don’t have access to a dedicated GPU and usually rely on Colab’s free GPU to train my models. . I emailed the TFRC team to ask if I could get upgraded from $5$ separate individual TPUv3’s (with 8 cores each) to a TPU pod to pretrain a large language model. The very next day (!) I got an email back saying that I could get access to a preemptible 128-core TPUv3 Pod for 7 days which unfortunately wasn’t long enough for me to pretrain the $1.5$B parameter model but was enough to train a few runs on the $124$M model. . . Setup . . So for setup I’ll be going through all the steps that I took to setup my VM and TPU Pod and preprocess the dataset as well. . When I was working on this project, I set up two VMs; One with a lot of RAM and CPU cores to process the data quickly and another small instance to run the TPU training script. One of the nice things about training on TPUs and TPU pods is that as long as your data has been preprocessed as a set of TFRecord files, you don’t need a really powerful VM instance which saves you a lot of money/compute credits. . You can look at this for a full list of every command that I used to setup the VM and preprocess the dataset. . . OpenWebText . . I used a n-1-standard-16 instance with TF2.1 to process the OpenWebText dataset. Make sure that you use an instance with a SSD instead of the default HDD because processing the dataset involves processing a lot of very small text files and is mostly limited by your drive’s io speed. I made the mistake of using a HDD and just extracting the dataset’s TAR archives took about 7 hours. I put all the data in a folder at ~/data/openwebtext/ so modify it if you want to download the data elsewhere. . TIL: most common linux utilities (like ls, mv, and cat) aren’t really that optimized for working with almost 10 million files like in OpenWebText. Just counting the number of text files in the dataset could take several minutes._ . Download the OpenWebText dataset (which is really just a tar archive of a bunch of tar archives that contain a lot of text files) and extract it: . gdown https://drive.google.com/uc?id=1EA5V0oetDCOke7afsktL_JDQ-ETtNOvx tar -xf openwebtext.tar.xz cat *.xz | tar -J -xf - -i . The dataset is about 12GB compressed and 53GB uncompressed and has just about 8 million text files. . I moved the first $100,000$ files in the dataset to a separate directory to create a validation set: . ls -f | head -100000 | xargs -i mv {} ../openwebtext-valid/ . . Tokenization . . I trained a Byte-level BPE tokenizer with a vocabulary size of $50,257$ (The same as GPT-2) on a $1$M file subset of the training set (I’m not sure if GPT-2 trains the tokenizer on the entire dataset or on just a subset, but I know that the CTRL paper trains their tokenizer on a 5% split of their training set.). I used Hugginface’s fast Rust-based Tokenizers library and their ByteLevelBPETokenizer tokenizer. . You can use my script here and run . python3 train_tokenizer.py --train_path ./data/openwebtext/ --save_path ./tokenizer/ --vocab_size 50257 --n_files 1000000 . to train the tokenizer, or just take a look at this for the main details (It just trains a tokenizer and saves it as well as a configuration file to disk): . import os import glob import json from tokenizers import ByteLevelBPETokenizer paths = glob.glob(os.path.join(&#39;./data/openwebtext&#39;, &#39;*&#39;))[:1000000] tok = ByteLevelBPETokenizer() tok.train(files=paths, vocab_size=args.vocab_size, special_tokens=args.control_codes) tok.save(&#39;./tokenizer/&#39;) tokenizer_config = { &quot;max_len&quot;: 1024 } with open(os.path.join(&#39;./tokenizer/&#39;, &quot;tokenizer_config.json&quot;), &#39;w&#39;) as fp: json.dump(tokenizer_config, fp) . . TFRecords . . TPU Pods expect your data to be available as a set of TFRecord files in a GCP cloud bucket that get downloaded to each of your TPU board’s built in powerful VM that will take care of de-serializing the files and feeding it to the TPU chips. Make sure that your GCP bucket and your TPU pod are in the same compute zone, otherwise you’ll quickly rack up a lot of charges by transferring hundreds of GBs of data across compute zones. . Here’s a thing that’s not very well documented when working with TPU Pods (this doesn’t really apply to TPUs as much): TPU Pods create a lot (100s of GBs) of logs that get sent to Stackdriver, where you get charged about 50 cents for each GiB of logs ingested beyond a certain limit (I think it’s around 50GiB/month). In just a few days of training, I ended up being charged about a $$100$ IIRC. Luckily, I still had most of the free GCP credits so this didn’t end up being a major problem for me, but make sure to turn off ingesting logs for TPUs. . I ran into a problem early on when I got access to the TPU pod where my code would work perfectly on a single TPU, but would throw an Out of range: End of sequence error when running it on a TPU pod. I struggled with this for a pretty long time until I took a look at this Kaggle discussion post that says that TPUs expect each TPU board (8 cores) to get its own TFrecord file (until that point, I was splitting the train set into 8 TFRecord files where I should’ve been splitting it into 16 (128 cores / 8 cores per board) TFRecord files. . TPUs are awesome for scaling to huge models and huge datasets, but there are a lot of TPU-specific information (especially for TPU Pods) that you need to know that’s not covered in the documentation and isn’t easy to find._** . You can use my script here and run . python3 make_tfrecords.py --path ./data/openwebtext/ --save_path ./train/ --files_per_tfrecord 500000 --use_control_codes --seq_len 1024 --min_seq_len --tokenizer ./tokenizer/ . python3 make_tfrecords.py --path ./data/openwebtext-valid/ --save_path ./val/ --files_per_tfrecord 50000 --use_control_codes --seq_len 1024 --min_seq_len --tokenizer ./tokenizer/ . to convert the raw text files from the train and validation splits into two sets of $16$ TFRecord files. . I ran a quick analysis on the average lengths of text fields in the dataset, $67$% of files have less than $1024$ tokens, $35$% of files have less than $512$ tokens, and only $10$% of files have less than $256$ tokens. This means that if I wanted to make the dataset as clean as possible and have each input sequence to the model be of a single contigous stream of $1024$ tokens, the dataset’s size would be a lot smaller. For this reason, everyone prepends a token like &lt;|endoftext|&gt; to the beginning of each sequence and concatenates together sequences with lengths smaller than $1024$. The specifics of how exactly you do that (e.g. do you treat the dataset as single stream of tokens and just break it up into sequences of length $1024$, or do you keep track of sequences smaller that $1024$ and just concatenate them together into a single sequence) really shouldn’t make too big of a difference in your model’s performance, but you can take a look at my implementation here. . My version doesn’t take full advantage of the fast, multithreaded batch_encode_plus() way to tokenize large datasets in parallel since it only keeps the first context_len tokens in each line of the files which makes dealing with files with more or less than $1024$ tokens harder. Because of this, tokenizing the dataset takes about $8$ hours, which is something I want to improve. . The train set comes out to about $26$GB and consists of about $8$M text files that have been transformed into just under $7$M tfrecord examples, each with $1024$ tokens (same as GPT-2). The validation set comes out to about $300$MB and consists of about $100$K text files that have been transformed into just about $90$K tfrecord examples, each with $1024$ tokens (also the same as GPT-2). . . # Code . . Since I’m using TPUs, the only real library that you can practically use right now would be Tensorflow. I didn’t want to have to go through the learning curve of learning how to make custom training loops and stuff in TF2 so I just stuck to using Keras. You can take a look at my training script (It’s pretty short) here. It’s pretty simple so I’m not going to copy over the entire training script, but I will talk about a few small code snippets. . I usually like to add a ptvsd breakpoint to my script so I can debug my training script locally with vscode before pushing it up to my VM . if args.debug: import ptvsd ptvsd.enable_attach(address=(&#39;localhost&#39;, 5678), redirect_output=True) ptvsd.wait_for_attach() breakpoint() . I’m using Weights&amp;Biases to keep track of my experiments and save checkpoints. . wandb.login() wandb.init(project=&#39;lm-finetuning&#39;, config=args, tags=args.tags) ... wandb_callback = WandbCallback(save_model=False) . Usually when you’re using a TPU with Keras, you pass in the IP address and port of the TPU to TPUClusterResolver, but you pass the name of the TPU itself to the resolver when using a TPU Pod. . resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu) tf.config.experimental_connect_to_cluster(resolver) tf.tpu.experimental.initialize_tpu_system(resolver) . . Replicating GPT-2 . . I tried to use as many of the original hyperparameters that OpenAI used when I was replicating their $124$M parameter version of GPT-2, but I had to modify a few things so I could train everything in time. . Note: For some reason, the authors of the GPT-2 paper don’t state exactly what learning rates they used for training their models and instead just state “The learning rate of each model was manually tuned for the best perplexity on a 5% held-out sample of WebText”. . OpenAI trains their models for a total of $800$K iterations at a batch size of $512$ (Which comes out to around a total of $60$ epochs through the training set). . I trained my GPT-2 model for $1/8th$ the number of iterations that OpenAI trained theirs for (a total of around $100$K iterations) since each $100$K iteration training run took about $20$ hours to run on my 128-core TPU Pod. If I wanted to train GPT-2 for the same number of iterations as OpenAI, a single training run would have used up most of my one week of access to the pod. . Since my TPU pod was preemptible and resets every $24$ hours I usually had to resume my training run at least once and is the reason why all of these graphs usually have two or more training runs on them. . . Replicating GPT-2 . . So here’s my model that came really close to replicating GPT-2, the training perplexity is about $21.5$ at the end of the almost $90$K training iterations. For comparison, GPT-2 gets a training perplexity about $17.5$ ppl after about $800$K training iterations, so a difference of only about $4$ ppl. . I made a colab notebook showing how to use my pretrained GPT-2 model to generate text . . AdamW vs Adafactor . . I wanted to use the memory-saving Adafactor optimizer to make it easier to train larger language models but all of my Adafactor training runs were a lot (~5ppl IIRC) worse than using AdamW (This may be due to not using Adafactor’s momentum parameter or relative update scale, so this is something I want to look into more soon). . . Learning rates . . I started out with using Adam’s default learning rate of $1e-4$ but I quickly figured out that I could train my models a lot faster by using a higher learning rate like $1e-3$. . Section 2 of the GPT-3 paper lists the learning rates the OpenAI team used for different sized models when training GPT-3. They use a learning rate of $6e-4$ for the $124$M version of their model and decrease the learning rate with model size. . You can take a look at this partial training run to see the difference between training at different learning rates. . . Pretraining ALGPT-2 . . Since I was using the Huggingface Transformers repository’s implementations of GPT-2 and ALBERT, I just forked the repository and modified a few files to implement my ALGPT-2 model. You can take a look at all the changes that I had to make here, most of the changes are only to make ALGPT-2 compatible with the /Transformers library and to be able to use the useful abstractions that it gives you, but most of the important code is in the modelling_algpt2.py file in which I just copied over the contents of modelling_gpt2.py and changed a few parts of the code. I’m only showing the changes that I made to the Pytorch version of ALGPT-2 here, the changes in the TF version are pretty similar to the Pytorch version and can be seen here. . . Implementing parameter sharing . . Implementing parameter sharing only involves changing a few lines of code: . class ALGPT2Model(ALGPT2PreTrainedModel): def __init__(self, config): super().__init__(config) ... - self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) - for _ in range(config.n_layer)]) + self.h = Block(config.n_ctx, config, scale=True) ... def forward(self, ...): ... if past is None: past_length = 0 - past = [None] * len(self.h) + past = [None] * self.config.n_layer ... - for i, (block, layer_past) in enumerate(zip(self.h, past)): + for i in range(self.config.n_layer): if self.output_hidden_states: all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),) - - outputs = block( + outputs = self.h( hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i], use_cache=use_cache, ) ... . . Implementing a factorized embedding . . Adding a factorized embedding is a little more work: . In the config.json that you use for your ALGPT-2 model, you need to specify that you want to use the ALGPT-2 and you need to specify the dimension of the factorized embedding that you want to use: . { + &quot;architectures&quot;: [&quot;ALGPT2LMHeadModel&quot;], &quot;attn_pdrop&quot;: 0.1, &quot;bos_token_id&quot;: 50256, &quot;embd_pdrop&quot;: 0.1, &quot;eos_token_id&quot;: 50256, &quot;initializer_range&quot;: 0.02, &quot;layer_norm_epsilon&quot;: 1e-5, + &quot;model_type&quot;: &quot;algpt2&quot;, &quot;n_ctx&quot;: 1024, &quot;n_embd&quot;: 768, &quot;n_head&quot;: 12, &quot;n_layer&quot;: 12, &quot;n_positions&quot;: 1024, &quot;resid_pdrop&quot;: 0.1, &quot;summary_activation&quot;: null, &quot;summary_first_dropout&quot;: 0.1, &quot;summary_proj_to_labels&quot;: true, &quot;summary_type&quot;: &quot;cls_index&quot;, &quot;summary_use_proj&quot;: true, &quot;vocab_size&quot;: 50257, + &quot;embedding_size&quot;: 128 } . Back in modelling_algpt2.py, define the two factorized embedding matrices (the first second matrix that is really just a simple linear layer) . class ALGPT2Model(ALGPT2PreTrainedModel): def __init__(self, config): super().__init__(config) ... - self.wte = nn.Embedding(config.vocab_size, config.n_embd) - self.wpe = nn.Embedding(config.n_positions, config.n_embd) + self.wte = nn.Embedding(config.vocab_size, config.embedding_size) + self.wpe = nn.Embedding(config.n_positions, config.embedding_size) + self.projection_layer = nn.Linear(config.embedding_size, config.n_embd) ... def forward(self, ...): ... hidden_states = inputs_embeds + position_embeds + token_type_embeds + hidden_states = self.projection_layer(hidden_states) ... class ALGPT2LMHeadModel(ALGPT2PreTrainedModel): def __init__(self, config): super().__init__(config) ... - self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) + self.dense = nn.Linear(config.n_embd, config.embedding_size) + self.lm_head = nn.Linear(config.embedding_size, config.vocab_size, bias=False) def forward(self, ...): ... - lm_logits = self.lm_head(hidden_states) + dense = self.dense(hidden_states) + lm_logits = self.lm_head(dense) ... . . Effect of layer-wise parameter sharing . . This version of ALGPT-2 has about $47$M parameters while GPT-2 has $124$M. This ALGPT-2 model with parameter sharing trains a lot faster than GPT-2 ($9$ hours vs $20$ hours for a $90$K iteration training run), but is consistently about $10$ ppl worse than GPT-2 ($31$ vs $21$ ppl). . This difference is quite a bit larger than the difference between ALBERT and BERT, but might be explained by masked language modelling being an easier task than autoregressive language modelling. Increasing the size of the ALGPT-2 model might make it more competitive with GPT-2. . . Effect of removing dropout . . I ran a partial training run on removing dropout from ALGPT-2. I didn’t run it for very long, but it looks like removing dropout gives you a slight improvement (~3ppl). . . Effect of factorized embeddings . . I ran three experiments for $90$K iterations with three different values for the factorized embedding ($128$, $256$, and $512$) as well as the baseline version without a factorized embedding. . Model ALGPT-2 ALGPT-2 512 ALGPT-2 256 ALGPT-2 128 . Parameters | 47M | 34M | 20M | 13M | . Time | ~9H | ~9H | ~9H | ~9H | . Perplexity | 31 | 31 | 34 | 38 | . There was practically no difference in the loss curves between the baseline and the $512$ run since the change in the number of parameters wasn’t that great. However, the training runs with factorized embeddings of sizes $256$ and $128$ were significantly worse than the baseline: $34$ and $38$ ppl respectively, a pretty big difference from the baseline of $31$ ppl. . . Effect of model size . . I only had the time to run one more full training run with ALGPT-2-medium (this one is comparable to the $345$M version of GPT-2). ALGPT-2-medium has about $66$M parameters and took twice as long as ALGPT-2 to train (a little more than $20$ hours). The larger model size made quite a big difference in performance, the training perplexity decreased $5$ppl from $31$ to $26$ ppl. . . Conclusion and next steps . . Well that’s pretty much everything that I did. After my TPU pod’s quota was used up, I started working on a few other things over the summer and just kept delaying writing up what I did for a couple of months until now. . There are a lot of things that I still want to work on or look into: . Training larger versions of ALGPT-2 | Removing or replacing the normalization layers in transformers | Working on distilling/shrinking language models with billions of parameters to make them more accessible | Apply something like PPLM to condition language models for few-shot inference (kinda like what GPT-3 does). | . Thanks for reading through all this. If you think there’s any mistakes or inaccuracies in this post, please let me know. .",
            "url": "https://bkkaggle.github.io/blog/algpt2/2020/07/17/ALGPT2-part-2.html",
            "relUrl": "/algpt2/2020/07/17/ALGPT2-part-2.html",
            "date": " • Jul 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Pytorch Zoo",
            "content": "I originally wrote this blog post for the PyTorch blog which is available here (Use this link to access my article with my friend link so you don’t need to worry about Medium paywalling my article) . . PyTorch Zoo is a collection of modules and utilities that I’ve found to be useful when working on machine learning projects and competitions. PyTorch Zoo contains several modules not available in PyTorch, like cyclical momentum and squeeze-and-excitation, as well as useful utilities like the ability to send notifications and set random seeds to get consistent results. PyTorch Zoo is meant to provide high-quality reference implementations of modules that don’t have official implementations in PyTorch and save you time that would have otherwise been spent searching for implementations on Github or coding the module yourself. . . &lt;/img&gt; . From: https://github.com/bkkaggle/pytorch_zoo . . The library is open-source on Github and is available as a pip package. Just run: . pip install pytorch_zoo . to install it in your local development environment and check out the documentation for in-depth examples on all the library’s features. I’ve included quite a few modules in PyTorch Zoo, so I’ll try to focus only on some of the ones that I found to be the most interesting for this blog post. . . Cyclical Momentum . . Cyclical momentum, which was first proposed in the same paper as cyclical learning rates 1, is usually used together with cyclical learning rates. It decreases the amount of momentum while the learning rate increases and increases the amount of momentum while the learning rate decreases, stabilizing training and allowing for the use of higher learning rates. Here’s an example of how you could use cyclical momentum just like a normal PyTorch scheduler: . optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) scheduler = torch.optim.CyclicMomentum(optimizer) data_loader = torch.utils.data.DataLoader(...) for epoch in range(10): for batch in data_loader: scheduler.batch_step() train_batch(...) . . Squeeze and Excitation . . Squeeze and Excitation modules 2 3 can be easily integrated into existing models by just adding one of these modules after each convolutional block and improves the model’s performance without significantly impacting training time. All three variants of the squeeze-and-excitation block that were proposed in the original papers are available in PyTorch Zoo, see the documentation for specific examples on how to use each one. Here’s an example of how you could use SqueezeAndExcitation in a convolutional block . class Encoder(nn.Module): def __init__(self, in_ch, out_ch, r): super(Encoder, self).__init__() self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1) self.se = SqueezeAndExcitation(out_ch, r) def forward(self, x): x = F.relu(self.conv(x), inplace=True) x = self.se(x) return x . . Utilities . . PyTorch Zoo also has a small range of utilities to make it easier to follow PyTorch best practices when doing things like saving a model to disk and setting random seeds, as well as easy to use one-liners to do things like sending push notifications when a training run ends. . Here’s an example of how you could use some of these utilities: . # Send a notification to your phone directly with IFTTT (https://ifttt.com/) notifying # you when a training run ends or at the end of an epoch. notify({&#39;value1&#39;: &#39;Notification title&#39;, &#39;value2&#39;: &#39;Notification body&#39;}, key=[IFTTT_KEY]) # Automatically set random seeds for Python, numpy, and Pytorch to make sure your results can be reproduced seed_envirionment(42) # Print how much GPU memory is currently allocated gpu_usage(device, digits=4) # GPU Usage: 6.5 GB # Print out the number of parameters in a Pytorch model print(n_params(model)) # 150909673 # Save a model for a particular cross-validation fold to disk save_model(model, fold=0) . . Conclusion . . To learn more about PyTorch Zoo and its features, check out our Github repository. . The project is still a work in progress, so if you find a bug, think there is something missing, or have any suggestions for new features or modules, feel free to open an issue or a pull request. Feel free to use the library or code from it in your own projects, and if you feel that some code used in this project hasn’t been properly accredited, please open an issue. . . References . . https://arxiv.org/abs/1803.09820 &#8617; . | https://arxiv.org/abs/1709.01507 &#8617; . | https://arxiv.org/abs/1803.02579 &#8617; . |",
            "url": "https://bkkaggle.github.io/blog/ai/cross-posts/2020/04/30/pytorch-zoo.html",
            "relUrl": "/ai/cross-posts/2020/04/30/pytorch-zoo.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Perplexity",
            "content": "Updated on Aug 2, 2020: Add link to more resources . . Purpose . . The purpose of these series of blog posts is to be a place to store my (still in-progress!) notes about topics in learning, help me keep track of everything I’ve learned over the last three years, and to practice my Latex skills. . This is my second blog post in the series, and this time I’m taking notes on evaluation metrics in NLP. . Most of the content of this post comes from Chip Huyen’s really good article in The Gradient on Evaluation methods for language models and the Deep Learning book, so a big thank you to the authors and editors for making this perplexing (pun intended) topic easy to understand. . let me be 100% clear here, I don’t want to come across like I’m taking someone else’s ideas and publishing them as my own. The purpose of this blog post is to take notes for myself so I can come back to this when I inevitably forget how to calculate perplexity. . Also, take a look at this for another good look at perplexity and the effect of tokenization on it. . . Background . . Language models like GPT2 try to predict the next word (or subword/character, we’ll use the term token in this blog post), in a context of tokens. . For example, when predicting the next word in the sentence &quot;I am a computer science and machine learning&quot;, the probability of the next work being enthusiast could be represented by . P(enthusiast∣I am a computer science and machine learning)P(enthusiast | I space am space a space computer space science space and space machine space learning)P(enthusiast∣I am a computer science and machine learning) . The probability of a sentence $s$, where $s$ is a sequence of n tokens $(w_{0}, w_{1}, … w_{n})$ can be represented as . P(s)=∏i=1np(wi∣w1...wi−1)P(s) = prod_{i = 1}^{n} p(w_i | w_1 ... w_{i-1})P(s)=i=1∏n​p(wi​∣w1​...wi−1​) . expanded, it looks like this: . P(s)=p(w1)p(w2∣w1)p(w3∣w1,w2)...p(wn∣w1w2...wn−1)P(s) = p(w_{1})p(w_{2} | w_{1})p(w_{3} | w_{1}, w_{2})...p(w_{n} | w_{1} w_{2} ... w_{n - 1})P(s)=p(w1​)p(w2​∣w1​)p(w3​∣w1​,w2​)...p(wn​∣w1​w2​...wn−1​) . . Information Theory . . The amount of information given by a discrete event $x$ is calculated by the Self-Information equation 1 . I(x)=−log P(x)I(x) = -log space P(x)I(x)=−log P(x) . Information is normally written in one of two units, $nats$, in which case the logarithm has a base of $e$ or $bits$, with a base of $2$. . One $nat$ encodes the “amount of information gained by observing an event with a probability of $ frac {1} {e}$.” 1 . . Shannon Entropy . . Shannon entropy is the extension of the Self-Information equation to probability distributions and is a way to “quantify the amount of uncertainty in an entire probability distribution.” 1 . H(x)=Ex∼P[log P(x)]H(x) = mathbb E_{x sim P} [log space P(x)]H(x)=Ex∼P​[log P(x)] . It’s a measure of how much information, on average is produced for each letter of a language 2 and (if calculated in units of $bits$) can also be defined as the average number of binary digits required to encode each letter in a vocabulary. . In NLP, the evaluation metric, Bits-per-character (BPC), is really just the entropy of a sequence, calculated with units of bits instead of nats. . Entropy calculated across language models that are trained over different context lengths aren’t exactly comparable, LMs with a longer context len will have more information from which to predict the next token. For example, given the sentence I work with machine learning it should be easier for a LM to predict the next word in the sequence I work with machine, than with just the first word: I. (This is actually a major pain point when I was trying to reproduce gpt2’s ppl numbers on wikitext2 and wikitext103, it’s still unclear how the paper evaluated the ppl values on the tests sets for both datasets.) . . Perplexity . . Perplexity: A measurement of how well a probability distribution or probability model predicts a sample 3 . Perplexity is usually calculated with units of $nats$, so calculate it with the equation: $PPL = e^{loss}$ . . Dealing with different tokenization schemes . . If you want to convert the perplexity between models that have been trained using different tokenization schemes and have a different number of tokens that the LM can predict, multiply the cross-entropy loss of the first language model by the ratio of $( text{n tokens first model} / text{n tokens seconds model})$ . The adjusted perplexity value can be found with 4 : . adj_ppl=eloss∗(#tokens/#tokens for other model)adj _ppl = e^{loss * ( text{ #tokens} / text{ #tokens for other model})}adj_ppl=eloss∗(#tokens/#tokens for other model) . . References . . Chapter 3, Deep Learning, Ian Goodfellow, Yoshua Bengio and Aaron Courville, 2016, MIT Press &#8617; &#8617;2 &#8617;3 . | Claude E Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):50–64, 1951. &#8617; . | https://en.wikipedia.org/wiki/Perplexity &#8617; . | https://github.com/NVIDIA/Megatron-LM/blob/master/evaluate_gpt2.py#L282 &#8617; . |",
            "url": "https://bkkaggle.github.io/blog/notes/2020/04/16/perplexity.html",
            "relUrl": "/notes/2020/04/16/perplexity.html",
            "date": " • Apr 16, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Normalization",
            "content": "Updated on Jun 26, 2020: Fix BatchNorm and LayerNorm equations . . Purpose . . The purpose of these series of blog posts is to be a place to store my (still in-progress!) notes about topics in machine learning, help me keep track of everything I’ve learned over the last three years, and to practice my Latex skills. . I’m starting this series of blog posts by writing down my notes on the different types of normalization in neural networks. Let’s see how this goes. . . Why normalize? . . Not normalizing input activations means that layers can transform activations to have very large or small means and standard deviations and cause the gradients to explode or vanish. . . Batch Normalization . . Arxiv . Tl;dr: Calculate the mean and standard deviation for each feature in the batch across the batch dimension and normalize to have a mean of $0$ and a standard deviation of $1$, then scale the resulting activations by two learned parameters $ gamma$ and $ beta$ . . For a mini-batch of activations B={x1...xm}, text{For a mini-batch of activations} space B = { { x_{1} ... x_{m} } },For a mini-batch of activations B={x1​...xm​}, . μB←1m∑i=1mxi mu_{B} leftarrow frac{1} {m} sum_{i=1}^{m} x_{i}μB​←m1​i=1∑m​xi​ . σB2←1m∑i=1m(xi−μB)2 sigma_{B}^{2} leftarrow frac{1} {m} sum_{i=1}^{m} (x_{i} - mu_{B}) ^ 2σB2​←m1​i=1∑m​(xi​−μB​)2 . x^←xi−μBσB2+ϵ hat{x} leftarrow frac {x_{i} - mu_{B}} { sqrt { sigma_{B}^{2} + epsilon}}x^←σB2​+ϵ . ​xi​−μB​​ . yi←γxi^+βy_{i} leftarrow gamma hat{x_{i}} + betayi​←γxi​^​+β . In Batch Normalization 1, you first calculate the mean and variance of the input tensor across the batch dimension, then subtract the input tensor by the mean $ mu_{B}$ and divide by the standard deviation (plus a small value to prevent dividing by $0$) $ sqrt { sigma_{B}^{2}}$ to restrict the activations of the neural network to having a mean of $0$ and a standard deviation of $1$ . You then scale the activations with learned parameters by rescaling the zero-mean activations by two learned parameters $ beta$ and $ gamma$. . The original paper claimed that the reason batch norm worked so well was by reducing internal covariate shift (“The change in the distribution of the input values to a learning algorithm” link), but more recent papers have disputed this and given other reasons to why it works so well. . This lets the network choose the mean and standard deviation that it wants for its activations before they are passed to a convolutional or fully connected layer. . One question that I’ve had over and over again related to batch norm is where exactly to place it in a network, and it looks like other people have had the same question. . The original paper places the batch norm layer after the convolutional layer and before the non-linearity, which is the default used by torchvision and other model zoos. It also claims that using batch norm can reduce or eliminate the need to use dropout, so the order could look like either of these: . Conv→BN→ReLU→DropoutConv rightarrow BN rightarrow ReLU rightarrow DropoutConv→BN→ReLU→Dropout . Conv→BN→ReLUConv rightarrow BN rightarrow ReLUConv→BN→ReLU . Some benchmarks show that placing the batch norm layer after the non-linearity can perform better 2 . Conv→ReLU→BN→DropoutConv rightarrow ReLU rightarrow BN rightarrow DropoutConv→ReLU→BN→Dropout . but this isn’t widely used. . One major disadvantage with this is that the pre-normalized activations must be saved for the backwards pass. This means that if you add a batchnorm layer for each convolutional layer in your network (which is a common practice), your network will need about twice the memory to store the same batch size into the GPU. Beyond using up more GPU memory, batch norm doesn’t work with batch sizes of 1, and doesn’t perform well with small batch sizes since the calculated mean and standard deviation for each batch will change a lot from batch to batch and gives the model a very noisy estimate of the true distribution. . Another thing you should keep in mind about batch norm is that when training on multiple gpus or machines, that by default, each gpu will keep its own mean and standard deviation parameters, which can be a problem if the per-gpu batch size is too low. There are synchronized batch norm implementations available that should fix this. Another thing to keep in mind is what mean and standard deviation values to use when evaluating on a test set or finetuning on a new dataset. . Other work, like In-Place Batch normalization 3 reduces the memory usage by recomputing the pre-batchnorm activations from the post-batchnorm activations, while others, like Fixup Initialization 4, MetaInit 5, LSUV 6, and Delta Orthogonal 7 use special initialization strategies to remove the need for batch normalization. . . Layer normalization . . Arxiv . Tl;dr: Calculate the mean and standard deviation for element in the batch across the feature dimension and normalize to have a mean of $0$ and a standard deviation of $1$, then scale the resulting activations by two learned parameters $ gamma$ and $ beta$ . . For activations in a batch of shape $x_{ij}$, where $i$ is the batch dimension and $j$ is the feature dimension (assuming this is a simple feedforward network), . μi←1m∑j=1mxij mu_{i} leftarrow frac{1} {m} sum_{j=1}^{m} x_{ij}μi​←m1​j=1∑m​xij​ . σi2←1m∑j=1m(xij−μi)2 sigma_{i}^{2} leftarrow frac{1} {m} sum_{j=1}^{m} (x_{ij} - mu_{i}) ^ 2σi2​←m1​j=1∑m​(xij​−μi​)2 . x^←xij−μiσi2+ϵ hat{x} leftarrow frac {x_{ij} - mu_{i}} { sqrt { sigma_{i}^{2} + epsilon}}x^←σi2​+ϵ . ​xij​−μi​​ . yij←γxij^+βy_{ij} leftarrow gamma hat{x_{ij}} + betayij​←γxij​^​+β . Layer Normalization 8, is almost identical to batch normalization except that layer norm normalizes across the feature dimension instead of the batch dimension. This means that layer norm calculates a mean and standard deviation value for for each element in the batch instead of for each feature over all elements in the batch. . Layer norm is used mostly for RNNs and Transformers and has the same GPU memory requirements as batch norm. . . Resources . . These are some of the amazing and very helpful blog posts, tutorials, and deep dives that have helped me learn about the topic and write this blog post. . https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/ | https://github.com/pytorch/pytorch/issues/1959 | . . References . . https://arxiv.org/abs/1502.03167 &#8617; . | https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md &#8617; . | https://arxiv.org/abs/1712.02616 &#8617; . | https://arxiv.org/abs/1901.09321 &#8617; . | https://openreview.net/pdf?id=SyeO5BBeUr &#8617; . | https://arxiv.org/abs/1511.06422 &#8617; . | https://arxiv.org/abs/1806.05393 &#8617; . | https://arxiv.org/abs/1607.06450 &#8617; . |",
            "url": "https://bkkaggle.github.io/blog/notes/2020/03/29/normalization.html",
            "relUrl": "/notes/2020/03/29/normalization.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "What Is Ai",
            "content": "Google Doc . This was originally a report I made for my ELA class that I’ve formatted into a blog post. Since I had to create the report in a specific way for the assignment, some of the information here has isn’t really relevant to most people who would be reading this blog post and is less technical. . Artificial Intelligence, or as it’s more commonly known, AI, has been said to either transform our world into a utopia, or bring about our doom. With so many widely publicized news stories about AI systems that can generate convincingly human-like text, defeat world champions at board games that were previously thought to be too hard for a computer, or generate images of human faces that appear indistinguishable from the real things, it can seem that we are close to a point at which AI may become self-aware and pose a threat to humans. . To truly understand if these fears of AI surpassing human intelligence and taking over the world are justified, we must first know, what is AI? . What is AI? What is AI? For some, it conjures up pictures from movies like Terminator and The Matrix, of robots gaining sentience and taking over the world. The term “Artificial Intelligence” was coined in 1956, by computer scientist John McCarthy, who, in his article, defines AI as “The science and engineering of making intelligent machines, especially computer programs” 1. Other people define AI in similar terms, saying that “AI is a collection of methods and ideas for building software that can do some of the things that humans can do with their brains” 2. . What is the history of AI? The field of AI started in the years after the end of World War II. In 1947, the famous British mathematician and code-breaker Alan Turing gave a lecture on programming computers to develop intelligent machines 1. Turing was also the creator of the Turing test, a test to determine “a machine’s ability to exhibit intelligent behavior similar to that of a human.” . In the 1960s, researchers at MIT developed a chatbot (a chatbot is a computer program that attempts to carry on a conversation with a human) called ELIZA which was able to pass the Turing test and show that it is possible for a computer program to create human-like text. . In the 1970s and 80s, neural networks, a family of algorithms that are loosely based on the neurons in a human brain, were developed. Neural networks excelled at learning patterns from large amounts of data and were used to automate tasks like reading addresses from envelopes. . In the 1990s and 2000s more progress was made in solving large problems in AI. In 1996, IBM’s Deep Blue computer beat the world’s best chess player, and in 2000, Honda released ASIMO, a humanoid robot that was capable of walking and recognizing objects and gestures. . In 2010, IBM’s Watson computer beat the best human competitors on the trivia game show Jeopardy!. Since around 2012, a lot of AI research is being done in machine learning - deep learning in particular. Deep learning involves stacking layers of neural networks on top of each other to create “deep” neural networks. Neural networks are now used in most of the widely used AI applications today - digital assistants like Siri and Alexa, self-driving cars, and recommendation algorithms from Netflix, Youtube, and other social media companies are all using neural networks in some part. Neural networks currently work better than other AI techniques in many areas because of their ability to learn from large amounts of data and because of the increasing amount of computational power available to train them 3 . . What are the different types of AI? AI is not one single area of research, it consists of many different branches that each have different views on how to build artificially intelligent systems. There are two main types of artificial intelligence, narrow AI and general AI. 2 . Most of the advances in AI have been in narrow AI: getting computers to learn how to do certain tasks as good as or better than a human. Although computers can now do certain tasks better than humans, narrow AI systems are highly specialized - a system designed for classifying images can’t be used to control a robot arm 2 . One example of a branch of narrow AI would be Machine Learning, or ML. Machine learning involves teaching a computer to iteratively learn to solve a task by giving it a large amount of data to learn from. In this way, machine learning lets computers learn how to do tasks without explicitly giving it instructions on how to do so 3 . . General AI involves computers that can generalize to a wide variety of tasks like humans do. So far, there has been very little progress on developing general AI, so any general AI systems are very likely decades away, if not more. 2 . In what areas is AI being used? AI is being used by researchers in a wide variety of areas and for a wide variety of purposes. AI is being used in healthcare to predict the spread of the Coronavirus epidemic, predict with radiologist-level accuracy whether a person has cancer from an x-ray scan, and to more accurately predict the folded structure of proteins, which is a crucial step in designing new life-saving medicines. . AI is also being used by companies in three main ways. First, AI is being used for RPA (Robotic Process Automation), automating time-consuming administrative tasks like transferring data from emails to spreadsheets and databases. Second, AI is being used to gain cognitive insights (which involves using algorithms to “detect patterns in vast volumes of data and interpret their meaning”) by predicting what items customers will buy next and identifying credit card fraud in real time. Finally, AI is being used for cognitive engagement (using AI to engage with potential customers) with chatbots providing customer service at any time and creating customized care plans. 4 . Conclusion Artificial intelligence today is limited to computers that can do certain tasks, sometimes as good as or even better than what a human could do, but highly specialized and limited to the scope of the task that it was trained to do. Once one knows the limitations of AI as we have it today, the claims that AI is close to surpassing human intelligence and taking over the world seem unfounded. . Footnotes . http://jmc.stanford.edu/articles/whatisai/whatisai.pdf &#8617; &#8617;2 . | https://www.skynettoday.com/editorials/ai-coverage-best-practices &#8617; &#8617;2 &#8617;3 &#8617;4 . | https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/ &#8617; &#8617;2 . | Davenport, Thomas H and Ronanki, Rajeev. “Artificial Intelligence for the real world.” Harvard Business Review. January-February 2018: Pages 110 and 112 &#8617; . |",
            "url": "https://bkkaggle.github.io/blog/ai/non-technical/2020/03/22/what-is-ai.html",
            "relUrl": "/ai/non-technical/2020/03/22/what-is-ai.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bkkaggle.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Template",
            "content": "",
            "url": "https://bkkaggle.github.io/blog/2020/01/14/template.html",
            "relUrl": "/2020/01/14/template.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Check out my website for more about me. .",
          "url": "https://bkkaggle.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bkkaggle.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}